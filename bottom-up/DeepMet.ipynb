{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yKJq5TizC7Vw"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My Drive/Colab Notebooks/DeepMet/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY86ZfqtDHG0"
      },
      "outputs": [],
      "source": [
        "!pip install q numpy pandas tensorflow[and-cuda]==2.15.1 tf_keras==2.15.1 torch transformers==4.37.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1RY_H9GDMEk"
      },
      "outputs": [],
      "source": [
        "TF_USE_LEGACY_KERAS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WAX-UK4HE0oz"
      },
      "outputs": [],
      "source": [
        "import os, argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy.stats import spearmanr\n",
        "from math import floor, ceil\n",
        "# from transformers import *\n",
        "from transformers import RobertaTokenizer, RobertaConfig, TFRobertaModel\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "def preprocssing(x):\n",
        "    x = str(x)\n",
        "    x = '\"'+x+'\"'\n",
        "    return x\n",
        "\n",
        "\n",
        "def _convert_to_transformer_inputs(instance, instance2, tokenizer, max_sequence_length):\n",
        "    \"\"\"Converts tokenized input to ids, masks and segments for transformer (including bert)\"\"\"\n",
        "    def return_id(str1, str2, truncation_strategy, length):\n",
        "        inputs = tokenizer.encode_plus(str1, str2,\n",
        "                                       add_special_tokens=True,\n",
        "                                       max_length=length,\n",
        "                                       return_token_type_ids=True,\n",
        "                                       truncation_strategy=truncation_strategy)\n",
        "        input_ids = inputs[\"input_ids\"]\n",
        "        input_masks = [1] * len(input_ids)\n",
        "        input_segments = inputs[\"token_type_ids\"]\n",
        "        padding_length = length - len(input_ids)\n",
        "        padding_id = tokenizer.pad_token_id\n",
        "        input_ids = input_ids + ([padding_id] * padding_length)\n",
        "        input_masks = input_masks + ([0] * padding_length)\n",
        "        input_segments = input_segments + ([0] * padding_length)\n",
        "        return [input_ids, input_masks, input_segments]\n",
        "    input_ids, input_masks, input_segments = return_id(\n",
        "        instance, None, 'longest_first', max_sequence_length)\n",
        "    input_ids2, input_masks2, input_segments2 = return_id(\n",
        "        instance2, None, 'longest_first', max_sequence_length)\n",
        "    return [input_ids, input_masks, input_segments,\n",
        "            input_ids2, input_masks2, input_segments2]\n",
        "\n",
        "\n",
        "def compute_input_arrays(df, columns, tokenizer, max_sequence_length):\n",
        "    input_ids, input_masks, input_segments = [], [], []\n",
        "    input_ids2, input_masks2, input_segments2 = [], [], []\n",
        "    for _, instance in tqdm(df[columns].iterrows()):\n",
        "        ids, masks, segments, ids2, masks2, segments2 = \\\n",
        "            _convert_to_transformer_inputs(str(instance.sentence), str(instance.sentence2), tokenizer, max_sequence_length)\n",
        "        input_ids.append(ids)\n",
        "        input_masks.append(masks)\n",
        "        input_segments.append(segments)\n",
        "        input_ids2.append(ids2)\n",
        "        input_masks2.append(masks2)\n",
        "        input_segments2.append(segments2)\n",
        "    return [np.asarray(input_ids, dtype=np.int32),\n",
        "            np.asarray(input_masks, dtype=np.int32),\n",
        "            np.asarray(input_segments, dtype=np.int32),\n",
        "            np.asarray(input_ids2, dtype=np.int32),\n",
        "            np.asarray(input_masks2, dtype=np.int32),\n",
        "            np.asarray(input_segments2, dtype=np.int32)]\n",
        "\n",
        "\n",
        "def compute_output_arrays(df, columns):\n",
        "    return np.asarray(df[columns].astype(int))\n",
        "\n",
        "# Siamese structure\n",
        "def create_model():\n",
        "    input_id = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    input_mask = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    input_atn = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    input_id2 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    input_mask2 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    input_atn2 = tf.keras.layers.Input((MAX_SEQUENCE_LENGTH,), dtype=tf.int32)\n",
        "    config = RobertaConfig.from_pretrained('roberta-base', output_hidden_states=False)\n",
        "    base_model = TFRobertaModel.from_pretrained('roberta-base', config=config)\n",
        "    TransformerA = base_model(input_id, attention_mask=input_mask, token_type_ids=input_atn)[0]\n",
        "    TransformerB = base_model(input_id2, attention_mask=input_mask2, token_type_ids=input_atn2)[0]\n",
        "    output = tf.keras.layers.GlobalAveragePooling1D()(TransformerA)\n",
        "    output2 = tf.keras.layers.GlobalAveragePooling1D()(TransformerB)\n",
        "    x = tf.keras.layers.Concatenate()([output, output2])\n",
        "    x = tf.keras.layers.Dropout(DROPOUT_RATE)(x)\n",
        "    x = tf.keras.layers.Dense(2, activation='softmax')(x)\n",
        "    model = tf.keras.models.Model(inputs=[input_id, input_mask, input_atn, input_id2, input_mask2, input_atn2], outputs=x)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_Kho23PE3cd"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(suppress=True)\n",
        "!nvidia-smi\n",
        "print(tf.__version__)\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "print(\"GPUs:\", gpus)\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "# Experimental environment: Titan RTX\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3,4,5,6,7\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vRSMkH5ZFS90"
      },
      "outputs": [],
      "source": [
        "import os, argparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold, StratifiedKFold\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from scipy.stats import spearmanr\n",
        "from math import floor, ceil\n",
        "from transformers import RobertaTokenizer, RobertaConfig, TFRobertaModel\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Checkpoint directory\n",
        "    checkpoint_dir = './model/checkpoints'\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Hyperparameter search range:\n",
        "    # MAX_SEQUENCE_LENGTH = 64,128,256,512\n",
        "    # HIDDEN_SIZE = 768,1024\n",
        "    # RANDOM_STATE = 2020\n",
        "    # EPOCHS = 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20\n",
        "    # N_FOLD = 2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20\n",
        "    # BATCH_SIZE = 16,32,64,128,256,512\n",
        "    # DROPOUT_RATE = 0.1,0.2,0.3,0.4,0.5\n",
        "    # VALIDATION_SPLIT = 0.1,0.2,0.3\n",
        "    # LEARNING_RATE = 1e-5,2e-5,3e-5,4e-5,5e-5\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    parser.add_argument(\"--max_sequence_length\", default=128, type=int, required=False)\n",
        "    parser.add_argument(\"--hidden_size\", default=768, type=int, required=False)\n",
        "    parser.add_argument(\"--random_state\", default=2020, type=int, required=False)\n",
        "    parser.add_argument(\"--epochs\", default=3, type=int, required=False)\n",
        "    parser.add_argument(\"--n_fold\", default=10, type=int, required=False)\n",
        "    parser.add_argument(\"--batch_size\", default=24, type=int, required=False)\n",
        "    parser.add_argument(\"--dropout_rate\", default=0.2, type=float, required=False)\n",
        "    parser.add_argument(\"--validation_split\", default=0.1, type=float, required=False)\n",
        "    parser.add_argument(\"--learning_rate\", default=1e-5, type=float, required=False)\n",
        "    # args = parser.parse_args()\n",
        "    args = parser.parse_args(args=[])\n",
        "    MAX_SEQUENCE_LENGTH = args.max_sequence_length\n",
        "    HIDDEN_SIZE = args.hidden_size\n",
        "    RANDOM_STATE = args.random_state\n",
        "    EPOCHS = args.epochs\n",
        "    N_FOLD = args.n_fold\n",
        "    BATCH_SIZE = args.batch_size\n",
        "    DROPOUT_RATE = args.dropout_rate\n",
        "    VALIDATION_SPLIT = args.validation_split\n",
        "    LEARNING_RATE = args.learning_rate\n",
        "\n",
        "    tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "    train = pd.read_csv('./data/VUA/VUA_train_features2.csv')\n",
        "    test = pd.read_csv('./data/VUA/VUA_verb_features.csv')\n",
        "    test2 = pd.read_csv('./data/VUA/VUA_allpos_features.csv')\n",
        "    print('train shape =', train.shape)\n",
        "    print('verb test shape =', test.shape)\n",
        "    print('allpos test shape =', test2.shape)\n",
        "\n",
        "    train['sentence'] = train.sentence.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + train.word.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + train.pos.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + train.tag.apply(lambda x: preprocssing(x))\n",
        "    test['sentence'] = test.sentence.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + test.word.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + test.pos.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + test.tag.apply(lambda x: preprocssing(x))\n",
        "    test2['sentence'] = test2.sentence.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + test2.word.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + test2.pos.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + test2.tag.apply(lambda x: preprocssing(x))\n",
        "    train['sentence2'] = train.local.apply(lambda x: preprocssing(x)) \\\n",
        "                          + \"[SEP]\" + train.word.apply(lambda x: preprocssing(x)) \\\n",
        "                          + \"[SEP]\" + train.pos.apply(lambda x: preprocssing(x)) \\\n",
        "                          + \"[SEP]\" + train.tag.apply(lambda x: preprocssing(x))\n",
        "    test['sentence2'] = test.local.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + test.word.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + test.pos.apply(lambda x: preprocssing(x)) \\\n",
        "                        + \"[SEP]\" + test.tag.apply(lambda x: preprocssing(x))\n",
        "    test2['sentence2'] = test2.local.apply(lambda x: preprocssing(x)) \\\n",
        "                          + \"[SEP]\" + test2.word.apply(lambda x: preprocssing(x)) \\\n",
        "                          + \"[SEP]\" + test2.pos.apply(lambda x: preprocssing(x)) \\\n",
        "                          + \"[SEP]\" + test2.tag.apply(lambda x: preprocssing(x))\n",
        "\n",
        "    # Track the last completed fold and epoch\n",
        "    last_completed_fold = 0\n",
        "    last_completed_epoch = 0\n",
        "\n",
        "    # Find the last saved checkpoint\n",
        "    for fold in range(N_FOLD):\n",
        "        for epoch in range(1, EPOCHS+1):\n",
        "            checkpoint_path = os.path.join(checkpoint_dir, f\"model-{fold}-{epoch}.h5\")\n",
        "            if os.path.exists(checkpoint_path):\n",
        "                last_completed_fold = fold\n",
        "                last_completed_epoch = epoch\n",
        "            else:\n",
        "                break  # Move to the next fold if checkpoint for this epoch is missing\n",
        "        if last_completed_epoch < EPOCHS:\n",
        "            break  # Stop if a fold didn't complete all epochs\n",
        "\n",
        "    # Resume training from the last checkpoint\n",
        "    start_fold = last_completed_fold + 1 if last_completed_epoch == EPOCHS else last_completed_fold\n",
        "    start_epoch = 0 if last_completed_epoch == EPOCHS else last_completed_epoch\n",
        "    print(start_fold, start_epoch)\n",
        "\n",
        "    input_categories = ['sentence', 'sentence2']\n",
        "    output_categories = 'label'\n",
        "    inputs = compute_input_arrays(train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "    outputs = compute_output_arrays(train, output_categories)\n",
        "    test_inputs = compute_input_arrays(test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "    test_inputs2 = compute_input_arrays(test2, input_categories, tokenizer, MAX_SEQUENCE_LENGTH)\n",
        "\n",
        "    pred = np.zeros((len(test)))\n",
        "    pred2 = np.zeros((len(test2)))\n",
        "    gkf = StratifiedKFold(n_splits=N_FOLD, random_state=RANDOM_STATE, shuffle=True).split(X=train[input_categories], y=train[output_categories])\n",
        "\n",
        "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
        "        if fold < start_fold:\n",
        "            continue  # Skip folds that were already completed\n",
        "\n",
        "        # Data preparation\n",
        "        train_inputs = [inputs[i][train_idx] for i in range(len(inputs))]\n",
        "        train_outputs = to_categorical(outputs[train_idx])\n",
        "        valid_inputs = [inputs[i][valid_idx] for i in range(len(inputs))]\n",
        "        valid_outputs = to_categorical(outputs[valid_idx])\n",
        "\n",
        "        # Checkpoint path for the current fold and epoch\n",
        "        checkpoint_path = os.path.join(checkpoint_dir, f\"model-{last_completed_fold}-{last_completed_epoch}.h5\")\n",
        "\n",
        "        # Load weights if checkpoint exists\n",
        "        if os.path.exists(checkpoint_path) and start_epoch < EPOCHS:\n",
        "            print(f\"Loading weights from checkpoint: {checkpoint_path}\")\n",
        "            model = create_model()\n",
        "            model.load_weights(checkpoint_path)\n",
        "        else:\n",
        "            model = create_model()\n",
        "\n",
        "        # # Reset start_epoch for subsequent folds\n",
        "        if fold > start_fold:\n",
        "            start_epoch = 0\n",
        "\n",
        "        # Modify the callback to save checkpoints for each epoch\n",
        "        cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "            filepath=os.path.join(checkpoint_dir, f\"model-{fold}-{epoch}.h5\"),\n",
        "            save_weights_only=True,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        # Train the model for the remaining epochs\n",
        "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=LEARNING_RATE)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc', 'mae'])\n",
        "        model.fit(\n",
        "            train_inputs,\n",
        "            train_outputs,\n",
        "            validation_data=[valid_inputs, valid_outputs],\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            initial_epoch=start_epoch,  # Start from the correct epoch\n",
        "            callbacks=[cp_callback]\n",
        "        )\n",
        "\n",
        "        fold_pred = np.argmax(model.predict(test_inputs), axis=1)\n",
        "        fold_pred2 = np.argmax(model.predict(test_inputs2), axis=1)\n",
        "        pred += fold_pred\n",
        "        pred2 += fold_pred2\n",
        "        print(\"folds: {:d}\".format(fold))\n",
        "        print(\"verb accuracy: {:.4f}\".format(accuracy_score(test.label, fold_pred)))\n",
        "        print(\"verb precision: {:.4f}\".format(precision_score(test.label, fold_pred)))\n",
        "        print(\"verb recall: {:.4f}\".format(recall_score(test.label, fold_pred)))\n",
        "        print(\"verb f1: {:.4f}\".format(f1_score(test.label, fold_pred)))\n",
        "        print(\"allpos accuracy: {:.4f}\".format(accuracy_score(test2.label, fold_pred2)))\n",
        "        print(\"allpos precision: {:.4f}\".format(precision_score(test2.label, fold_pred2)))\n",
        "        print(\"allpos recall: {:.4f}\".format(recall_score(test2.label, fold_pred2)))\n",
        "        print(\"allpos f1: {:.4f}\".format(f1_score(test2.label, fold_pred2)))\n",
        "\n",
        "\n",
        "    best_pred = np.zeros((len(test)))\n",
        "    best_score = 0\n",
        "    best_threshold = 0\n",
        "    for i in range(N_FOLD):\n",
        "        temp_pred = (np.array(pred) >= i).astype('int')\n",
        "        print(\"verb metaphor preference parameter alpha: {:d}\".format(i/N_FOLD))\n",
        "        print(\"verb accuracy: {:.4f}\".format(accuracy_score(test.label, temp_pred)))\n",
        "        print(\"verb precision: {:.4f}\".format(precision_score(test.label, temp_pred)))\n",
        "        print(\"verb recall: {:.4f}\".format(recall_score(test.label, temp_pred)))\n",
        "        print(\"verb f1: {:.4f}\".format(f1_score(test.label, temp_pred)))\n",
        "        if f1_score(test.label, temp_pred) > best_score:\n",
        "            best_score = f1_score(test.label, temp_pred)\n",
        "            best_pred = temp_pred\n",
        "            best_threshold = i\n",
        "\n",
        "    print(\"best verb metaphor preference parameter alpha: {:d}\".format(best_threshold/N_FOLD))\n",
        "    print(\"best verb accuracy: {:.4f}\".format(accuracy_score(test.label, best_pred)))\n",
        "    print(\"best verb precision: {:.4f}\".format(precision_score(test.label, best_pred)))\n",
        "    print(\"best verb recall: {:.4f}\".format(recall_score(test.label, best_pred)))\n",
        "    print(\"best verb f1: {:.4f}\".format(f1_score(test.label, best_pred)))\n",
        "\n",
        "    best_pred2 = np.zeros((len(test2)))\n",
        "    best_score2 = 0\n",
        "    best_threshold2 = 0\n",
        "    for i in range(N_FOLD):\n",
        "        temp_pred2 = (np.array(pred2) >= i).astype('int')\n",
        "        print(\"allpos metaphor preference parameter alpha: {:d}\".format(i/N_FOLD))\n",
        "        print(\"allpos accuracy: {:.4f}\".format(accuracy_score(test2.label, temp_pred2)))\n",
        "        print(\"allpos precision: {:.4f}\".format(precision_score(test2.label, temp_pred2)))\n",
        "        print(\"allpos recall: {:.4f}\".format(recall_score(test2.label, temp_pred2)))\n",
        "        print(\"allpos f1: {:.4f}\".format(f1_score(test2.label, temp_pred2)))\n",
        "        if f1_score(test2.label, temp_pred2) > best_score2:\n",
        "            best_score2 = f1_score(test2.label, temp_pred2)\n",
        "            best_pred2 = temp_pred2\n",
        "            best_threshold2 = i\n",
        "\n",
        "    print(\"best allpos metaphor preference parameter alpha: {:d}\".format(best_threshold2/N_FOLD))\n",
        "    print(\"best allpos accuracy: {:.4f}\".format(accuracy_score(test2.label, best_pred2)))\n",
        "    print(\"best allpos precision: {:.4f}\".format(precision_score(test2.label, best_pred2)))\n",
        "    print(\"best allpos recall: {:.4f}\".format(recall_score(test2.label, best_pred2)))\n",
        "    print(\"best allpos f1: {:.4f}\".format(f1_score(test2.label, best_pred2)))\n",
        "\n",
        "    test['predict'] = best_pred\n",
        "    test2['predict'] = best_pred2\n",
        "    print(test.predict.value_counts())\n",
        "    print(test2.predict.value_counts())\n",
        "\n",
        "    test[['id', 'sentence', 'word', 'label', 'predict']].to_csv('./predict/VUA_verb_predict.csv', index=False)\n",
        "    test2[['id', 'sentence', 'word', 'label', 'predict']].to_csv('./predict/VUA_allpos_predict.csv', index=False)\n",
        "    test[['id', 'predict']].to_csv('./submit/answer.txt', index=False, header=None)\n",
        "    test2[['id', 'predict']].to_csv('./submit/answer2.txt', index=False, header=None)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}